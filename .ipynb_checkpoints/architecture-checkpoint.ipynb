{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VisualBacter\n",
    "\n",
    "```\n",
    "Detecci√≥n de ETA's mediante vision computacional en redes nueronales convolucionales\n",
    "\n",
    "Escrito por:\n",
    "\n",
    "Juan Diego y George Giosue\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\george\\repos\\visualbacter\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "  File \"C:\\Users\\george\\AppData\\Local\\Temp\\ipykernel_19148\\1995618338.py\", line 3, in <module>\n",
      "    import tensorflow as tf\n",
      "ModuleNotFoundError: No module named 'tensorflow'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\george\\repos\\visualbacter\\venv\\Lib\\site-packages\\pygments\\styles\\__init__.py\", line 45, in get_style_by_name\n",
      "ModuleNotFoundError: No module named 'pygments.styles.default'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\george\\repos\\visualbacter\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2168, in showtraceback\n",
      "  File \"c:\\Users\\george\\repos\\visualbacter\\venv\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1454, in structured_traceback\n",
      "  File \"c:\\Users\\george\\repos\\visualbacter\\venv\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1345, in structured_traceback\n",
      "  File \"c:\\Users\\george\\repos\\visualbacter\\venv\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1192, in structured_traceback\n",
      "  File \"c:\\Users\\george\\repos\\visualbacter\\venv\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "  File \"c:\\Users\\george\\repos\\visualbacter\\venv\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1133, in get_records\n",
      "  File \"c:\\Users\\george\\repos\\visualbacter\\venv\\Lib\\site-packages\\pygments\\styles\\__init__.py\", line 47, in get_style_by_name\n",
      "pygments.util.ClassNotFound: Could not find style module 'pygments.styles.default', though it should be builtin.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from keras import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Layer\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense, Input, BatchNormalization, Permute, LSTM, Reshape, Concatenate, Lambda, Add\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from os import scandir\n",
    "from time import time\n",
    "\n",
    "\n",
    "import lib.utils as fns\n",
    "\n",
    "tf.__version__\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 11641322456663878896\n",
      "xla_global_id: -1\n",
      "] \n",
      "\n",
      " Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "\n",
    "print(device_lib.list_local_devices(), \"\\n\\n\", \"Num GPUs Available: \",len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Routes and File extensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"./data/train/\"\n",
    "TEST_DATA_PATH = \"./data/test/\"\n",
    "VALID_DATA_PATH = \"./data/test/\"\n",
    "\n",
    "TRAIN_CLASSES = {\n",
    "  \"filename\" : \"_classes.csv\",\n",
    "  \"fields\" : [],\n",
    "  \"rows\" : []\n",
    "}\n",
    "\n",
    "TEST_CLASSES = {\n",
    "  \"filename\" : \"_classes.csv\",\n",
    "  \"fields\" : [],\n",
    "  \"rows\" : []\n",
    "}\n",
    "\n",
    "VALID_CLASSES = {\n",
    "  \"filename\" : \"_classes.csv\",\n",
    "  \"fields\" : [],\n",
    "  \"rows\" : []\n",
    "}\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "valid_data = []\n",
    "\n",
    "with open(TRAIN_DATA_PATH+\"/\"+TRAIN_CLASSES['filename'], 'r') as csvfile:\n",
    "    # creating a csv reader object\n",
    "    csvreader = csv.reader(csvfile)\n",
    "  \n",
    "    # extracting field names through first row\n",
    "    columns = next(csvreader)\n",
    "    \n",
    "    for column in columns:\n",
    "      TRAIN_CLASSES['fields'].append(column.strip())\n",
    "  \n",
    "    # extracting each data row one by one\n",
    "    for row in csvreader:\n",
    "        TRAIN_CLASSES['rows'].append(row)\n",
    "\n",
    "\n",
    "with open(TEST_DATA_PATH+\"/\"+TEST_CLASSES['filename'], 'r') as csvfile:\n",
    "    # creating a csv reader object\n",
    "    csvreader = csv.reader(csvfile)\n",
    "  \n",
    "    # extracting field names through first row\n",
    "    columns = next(csvreader)\n",
    "    \n",
    "    for column in columns:\n",
    "      TEST_CLASSES['fields'].append(column.strip())\n",
    "  \n",
    "    # extracting each data row one by one\n",
    "    for row in csvreader:\n",
    "        TEST_CLASSES['rows'].append(row)\n",
    "\n",
    "with open(VALID_DATA_PATH+\"/\"+VALID_CLASSES['filename'], 'r') as csvfile:\n",
    "    # creating a csv reader object\n",
    "    csvreader = csv.reader(csvfile)\n",
    "  \n",
    "    # extracting field names through first row\n",
    "    columns = next(csvreader)\n",
    "    \n",
    "    for column in columns:\n",
    "      VALID_CLASSES['fields'].append(column.strip())\n",
    "  \n",
    "    # extracting each data row one by one\n",
    "    for row in csvreader:\n",
    "        VALID_CLASSES['rows'].append(row)\n",
    "\n",
    "# TRAIN_CLASES = data_frame\n",
    "def compose_images_with_class(payload, image_name, data_frame):\n",
    "  target_row = []\n",
    "  \n",
    "  for row in data_frame[\"rows\"]:\n",
    "    if row[0] == image_name:\n",
    "      target_row = row\n",
    "      \n",
    "\n",
    "  \n",
    "  features = data_frame[\"fields\"][1::] # ['Bacteria A', 'Stafilomandarina' ]\n",
    "  predictions = [int(p) for p in target_row[1::]] # [0, 0 , 1, 0]\n",
    "  \n",
    "  index_p = 0\n",
    "  prediction = ''\n",
    "  \n",
    "  for index, p in enumerate(predictions):\n",
    "    if p == 1:\n",
    "      index_p = index\n",
    "      \n",
    "  prediction = features[index_p]      \n",
    "  \n",
    "  \n",
    "  return {\n",
    "    \"image\": payload,\n",
    "    \"image_name\": image_name,\n",
    "    \"classname\": prediction\n",
    "  }\n",
    "\n",
    "CLASS_NAMES = ['Campylobacter', 'E.Coli', 'Staphylococcus', 'Streptococcus', 'Yeast']\n",
    "  \n",
    "train_data = fns.imgs_to_array(TRAIN_DATA_PATH,'_classes.csv',cv2.IMREAD_GRAYSCALE, lambda payload, image_name: compose_images_with_class(payload, image_name, TRAIN_CLASSES))\n",
    "\n",
    "test_data = fns.imgs_to_array(TEST_DATA_PATH, '_classes.csv', cv2.IMREAD_GRAYSCALE, lambda payload, image_name: compose_images_with_class(payload, image_name, TEST_CLASSES))\n",
    "\n",
    "valid_data = fns.imgs_to_array(TEST_DATA_PATH, '_classes.csv', cv2.IMREAD_GRAYSCALE, lambda payload, image_name: compose_images_with_class(payload, image_name, TEST_CLASSES))\n",
    "    \n",
    "print(f\"\"\"\n",
    "      SIZE OF TRAIN DATA: {len(train_data)}\n",
    "      SIZE OF TEST DATA: {len(test_data)}\n",
    "      \"\"\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Preview of Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_train_data_index = 100\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(train_data[preview_train_data_index]['image'])\n",
    "plt.colorbar()\n",
    "plt.xlabel(train_data[preview_train_data_index]['classname'])\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Preview of Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_test_data_index = 10\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(test_data[preview_test_data_index]['image'])\n",
    "plt.colorbar()\n",
    "plt.xlabel(test_data[preview_test_data_index]['classname'])\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (150, 150)\n",
    "\n",
    "# Model\n",
    "\n",
    "MODEL_PARAMETERS = {\n",
    "    'losses': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    'optimizer': 'adam',\n",
    "    'metrics': ['accuracy'],\n",
    "    'fit': {\n",
    "        'batches': {\n",
    "            'train': 0.7,\n",
    "            'test': 0.3,\n",
    "        },\n",
    "        'epochs': 10\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "INPUT_SHAPE = IMG_SHAPE + (1,)\n",
    "KERNEL_SIZE = (3, 3)\n",
    "\n",
    "activation = 'relu'\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, kernel_size = KERNEL_SIZE, activation = activation, padding = 'same', input_shape = INPUT_SHAPE))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = KERNEL_SIZE, padding = 'same', kernel_initializer = 'he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = KERNEL_SIZE, padding = 'same', kernel_initializer = 'he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = KERNEL_SIZE, padding = 'same', kernel_initializer = 'he_uniform'))\n",
    "model.add(BatchNormalization()) \n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation = activation, kernel_initializer = 'he_uniform'))\n",
    "model.add(Dense(len(CLASS_NAMES), activation = 'softmax'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sumary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model.summary()\n",
    "keras.utils.plot_model(model, 'model_arch.png', show_shapes=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=MODEL_PARAMETERS['losses'],\n",
    "              metrics=MODEL_PARAMETERS['metrics'],\n",
    "              run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CLASSES['fields'][1::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_grayscale_image(image_array, target_size=(150, 150)):\n",
    "    # Convert the image array to a TensorFlow tensor\n",
    "    image_tensor = tf.expand_dims(tf.convert_to_tensor(image_array, dtype=tf.float32), axis=0)\n",
    "    image_tensor = tf.expand_dims(image_tensor, axis=-1)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_image = tf.image.resize(image_tensor, target_size)\n",
    "\n",
    "    # Remove the batch dimension and the channel dimension\n",
    "    resized_image_array = tf.squeeze(resized_image).numpy()\n",
    "\n",
    "    return resized_image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [CLASS_NAMES.index(dc['classname']) for dc in train_data]\n",
    "test_labels = [CLASS_NAMES.index(dc['classname']) for dc in test_data]\n",
    "\n",
    "train_images = np.array([ resize_grayscale_image(dc['image']) for dc in train_data])\n",
    "test_images = np.array([ resize_grayscale_image(dc['image']) for dc in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_images, train_labels, epochs=32, validation_data=(test_images, test_labels), batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "print(test_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(history.history['loss'], color='teal', label='loss')\n",
    "plt.plot(history.history['val_loss'], color='orange', label='val_loss')\n",
    "fig.suptitle('Loss', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(history.history['accuracy'], color='teal', label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], color='orange', label='val_accuracy')\n",
    "fig.suptitle('Accuracy', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_valid_data_index = 10\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(valid_data[preview_valid_data_index]['image'])\n",
    "plt.colorbar()\n",
    "plt.xlabel(valid_data[preview_valid_data_index]['classname'])\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_images = np.array([ resize_grayscale_image(dc['image']) for dc in valid_data])\n",
    "\n",
    "valid_image_test = valid_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_image_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "\n",
    "predictions = model.predict(valid_image_test)\n",
    "\n",
    "index_max = np.argmax(predictions[0])\n",
    "index_min = np.argmin(predictions[0])\n",
    "\n",
    "print(f\"\"\"\n",
    "\n",
    "    The prediction is [MAX ARG]: {CLASS_NAMES[index_max\n",
    "    ][1]}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODELS_PATH = os.path.join(Path(os.getcwd()).parent.parent, 'models')\n",
    "\n",
    "MODEL_EXTENSION = \"h5\"\n",
    "\n",
    "MODEL_NAME = f\"model_{DATA_SET_EXTENSIBLE}_{IMG_SHAPE[0]}x_3_{str(round(test_acc, 4)*10000).split('.')[0]}_mc\"\n",
    "\n",
    "MODEL_FULL_NAME = f\"{MODEL_NAME}.{MODEL_EXTENSION}\"\n",
    "\n",
    "\n",
    "model.save(os.path.join(MODELS_PATH, MODEL_FULL_NAME))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded =tf.keras.models.load_model(os.path.join(MODELS_PATH, MODEL_NAME + '.' + MODEL_EXTENSION))\n",
    "\n",
    "\n",
    "#Convert to tflite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_loaded)\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "#Save the model.\n",
    "\n",
    "with open(f'{os.path.join(MODELS_PATH, MODEL_NAME)}.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7077c62dbc3425e67cc439db09f7291a8b966dee69dfb6e4d096cef9462d1e68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
